trial_id,config,error_type,error_message,error_traceback
af481dba,"{'latent_dim': 48, 'ae_topo_lambda': 2.836660698712587, 'ae_conv_num': 2, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.31295918062566064, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 245.06 MiB is free. Process 1096895 has 14.39 GiB memory in use. Process 1101359 has 22.33 GiB memory in use. Process 1101706 has 13.94 GiB memory in use. Process 1102113 has 14.10 GiB memory in use. Process 1102473 has 14.12 GiB memory in use. Of the allocated memory 11.71 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    result = h_search_unit(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
f1caee52,"{'latent_dim': 38, 'ae_topo_lambda': 2.340302752200631, 'ae_conv_num': 2, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.16182730941339796, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 247.06 MiB is free. Process 1096895 has 14.39 GiB memory in use. Process 1101359 has 22.33 GiB memory in use. Process 1101706 has 13.94 GiB memory in use. Process 1102113 has 14.10 GiB memory in use. Process 1102473 has 14.12 GiB memory in use. Of the allocated memory 11.71 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    result = h_search_unit(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
b3f32580,"{'latent_dim': 61, 'ae_topo_lambda': 2.9245880777838167, 'ae_conv_num': 3, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.132939759988781, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 239.06 MiB is free. Process 1096895 has 14.39 GiB memory in use. Process 1101359 has 22.33 GiB memory in use. Process 1101706 has 13.94 GiB memory in use. Process 1102113 has 14.10 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.72 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    result = h_search_unit(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
d3b567c5,"{'latent_dim': 58, 'ae_topo_lambda': 2.1454611848581875, 'ae_conv_num': 2, 'ae_conv_kernel': 2, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.4608977303988405, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 722.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 617.06 MiB is free. Process 1096895 has 14.02 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.94 GiB memory in use. Process 1102113 has 14.10 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 13.46 GiB is allocated by PyTorch, and 39.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
81c427bb,"{'latent_dim': 67, 'ae_topo_lambda': 2.2711097916347, 'ae_conv_num': 3, 'ae_conv_kernel': 2, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.34354434124498306, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 43.06 MiB is free. Process 1096895 has 14.55 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.94 GiB memory in use. Process 1102113 has 14.12 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 13.55 GiB is allocated by PyTorch, and 50.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
ae5760bd,"{'latent_dim': 50, 'ae_topo_lambda': 2.692740375736328, 'ae_conv_num': 2, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.49291115731935764, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 676.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 665.06 MiB is free. Process 1096895 has 13.99 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.94 GiB memory in use. Process 1102113 has 14.07 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.60 GiB is allocated by PyTorch, and 896.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
08d84a6e,"{'latent_dim': 68, 'ae_topo_lambda': 2.2403064406590243, 'ae_conv_num': 2, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.31433276647611885, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 676.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 223.06 MiB is free. Process 1096895 has 14.00 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.74 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.62 GiB is allocated by PyTorch, and 617.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
c297feef,"{'latent_dim': 9, 'ae_topo_lambda': 2.3385652771924392, 'ae_conv_num': 3, 'ae_conv_kernel': 2, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.08007770147507903, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 610.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 297.06 MiB is free. Process 1096895 has 14.00 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.67 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.33 GiB is allocated by PyTorch, and 1.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
074c9fae,"{'latent_dim': 49, 'ae_topo_lambda': 1.4342250527507108, 'ae_conv_num': 2, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.15235725848645185, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 297.06 MiB is free. Process 1096895 has 13.99 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.26 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
ddf4c629,"{'latent_dim': 28, 'ae_topo_lambda': 1.9855355544326667, 'ae_conv_num': 2, 'ae_conv_kernel': 2, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.4367526645579331, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 722.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 53.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 13.43 GiB is allocated by PyTorch, and 282.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
61ae205e,"{'latent_dim': 16, 'ae_topo_lambda': 0.7849001661066651, 'ae_conv_num': 2, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.16666717253406219, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 676.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 47.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.56 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
82797ce0,"{'latent_dim': 12, 'ae_topo_lambda': 2.384195193400884, 'ae_conv_num': 2, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.1418675043464298, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 338.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 55.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.23 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
c6efd14c,"{'latent_dim': 31, 'ae_topo_lambda': 2.74742328330754, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.19157063383285283, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 57.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.67 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.24 GiB is allocated by PyTorch, and 1.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
52b7ebb8,"{'latent_dim': 47, 'ae_topo_lambda': 2.222010741584303, 'ae_conv_num': 2, 'ae_conv_kernel': 2, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.24813785827033663, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 362.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 49.06 MiB is free. Process 1096895 has 14.24 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 13.09 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
a7d55e4d,"{'latent_dim': 31, 'ae_topo_lambda': 1.8619036881782247, 'ae_conv_num': 2, 'ae_conv_kernel': 2, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.21130473967516683, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 362.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 51.06 MiB is free. Process 1096895 has 14.24 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 13.07 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
54dd5fab,"{'latent_dim': 65, 'ae_topo_lambda': 2.7360592314518652, 'ae_conv_num': 1, 'ae_conv_kernel': 4, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.00017066206345567369, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 654.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 49.06 MiB is free. Process 1096895 has 14.24 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.19 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
12b955bd,"{'latent_dim': 55, 'ae_topo_lambda': 1.6462109794279771, 'ae_conv_num': 2, 'ae_conv_kernel': 4, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.24064613924059414, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 47.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.78 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
c03080e4,"{'latent_dim': 18, 'ae_topo_lambda': 2.5803274376207685, 'ae_conv_num': 2, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.30347773720659577, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 590.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 55.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.67 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 10.95 GiB is allocated by PyTorch, and 2.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
3adc804d,"{'latent_dim': 42, 'ae_topo_lambda': 2.94973300285218, 'ae_conv_num': 2, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.051373270567302995, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 59.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.67 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.26 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
e267f8bc,"{'latent_dim': 80, 'ae_topo_lambda': 1.1095631656027893, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.1456389325926678, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 678.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 57.06 MiB is free. Process 1096895 has 14.22 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.63 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
9483bdb1,"{'latent_dim': 62, 'ae_topo_lambda': 0.6841710600195955, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.2474191255330477, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 47.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 898.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
cc895599,"{'latent_dim': 69, 'ae_topo_lambda': 0.942655232413931, 'ae_conv_num': 1, 'ae_conv_kernel': 2, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.1825331392801336, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 654.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 61.06 MiB is free. Process 1096895 has 14.22 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.20 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
fab24ea4,"{'latent_dim': 78, 'ae_topo_lambda': 1.15192858383456, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.08463000978346147, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 57.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.67 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.80 GiB is allocated by PyTorch, and 1.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
8d633cfb,"{'latent_dim': 76, 'ae_topo_lambda': 0.7710033208326992, 'ae_conv_num': 2, 'ae_conv_kernel': 2, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.20114620613882064, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 53.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.67 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.80 GiB is allocated by PyTorch, and 1.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
9369ae5a,"{'latent_dim': 68, 'ae_topo_lambda': 0.4672164833457546, 'ae_conv_num': 2, 'ae_conv_kernel': 2, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.3680807718943385, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 364.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 63.06 MiB is free. Process 1096895 has 14.22 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.67 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 13.12 GiB is allocated by PyTorch, and 600.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
72604882,"{'latent_dim': 82, 'ae_topo_lambda': 1.1212216040570748, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.2554509376269768, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 81.06 MiB is free. Process 1096895 has 14.22 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.67 GiB memory in use. Process 1102113 has 14.67 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.29 GiB is allocated by PyTorch, and 1.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
d7f2c90a,"{'latent_dim': 84, 'ae_topo_lambda': 1.5214600660041995, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.24288138691121763, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 83.06 MiB is free. Process 1096895 has 14.22 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.67 GiB memory in use. Process 1102113 has 14.67 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.30 GiB is allocated by PyTorch, and 1.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
785cb6ad,"{'latent_dim': 68, 'ae_topo_lambda': 1.720689550683182, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.20066838227336187, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 63.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.67 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.79 GiB is allocated by PyTorch, and 1.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
6e03e63e,"{'latent_dim': 64, 'ae_topo_lambda': 1.1179141200724296, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.4023272352812566, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 55.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.78 GiB is allocated by PyTorch, and 1.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
f02d9f5e,"{'latent_dim': 75, 'ae_topo_lambda': 1.3398915563487455, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.23624634488624408, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 65.06 MiB is free. Process 1096895 has 14.22 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.80 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
99eb98a5,"{'latent_dim': 47, 'ae_topo_lambda': 0.8156968345351286, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.4474622822445321, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 57.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.77 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
991527a7,"{'latent_dim': 64, 'ae_topo_lambda': 1.09258354130401, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.36496047691608285, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 53.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.78 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
8312a9f7,"{'latent_dim': 39, 'ae_topo_lambda': 0.6067292488938953, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.325845498249988, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 61.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.67 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.76 GiB is allocated by PyTorch, and 1.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
d2c7784a,"{'latent_dim': 81, 'ae_topo_lambda': 1.0606596549851623, 'ae_conv_num': 1, 'ae_conv_kernel': 4, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.29215569182948214, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 612.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 67.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.67 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.40 GiB is allocated by PyTorch, and 1.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
788397ce,"{'latent_dim': 60, 'ae_topo_lambda': 0.34660391929565937, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.2585009084052265, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 49.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.78 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
79d966c9,"{'latent_dim': 37, 'ae_topo_lambda': 1.2381812284108518, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.13172425186822131, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 49.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.67 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.76 GiB is allocated by PyTorch, and 1.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
5ce5dfed,"{'latent_dim': 34, 'ae_topo_lambda': 0.7224499990589438, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.22689924907857087, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 49.06 MiB is free. Process 1096895 has 14.22 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.75 GiB is allocated by PyTorch, and 1.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
00884fb1,"{'latent_dim': 44, 'ae_topo_lambda': 1.1923379664252405, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.2105132411026303, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 59.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.68 GiB memory in use. Process 1102113 has 14.68 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.26 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
ba12e84e,"{'latent_dim': 77, 'ae_topo_lambda': 1.1336595952766584, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.17766125888250903, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 678.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 533.06 MiB is free. Process 1096895 has 14.23 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.21 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.62 GiB is allocated by PyTorch, and 66.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
ef6060c5,"{'latent_dim': 33, 'ae_topo_lambda': 0.591522965216613, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.1992810658926247, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 529.06 MiB is free. Process 1096895 has 14.22 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.22 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.75 GiB is allocated by PyTorch, and 1.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
dd575f7e,"{'latent_dim': 79, 'ae_topo_lambda': 1.34841082799667, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.4285798390375075, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 579.06 MiB is free. Process 1096895 has 14.22 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.17 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.80 GiB is allocated by PyTorch, and 870.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
7fd1f55c,"{'latent_dim': 53, 'ae_topo_lambda': 0.744521537840455, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.20825152525893686, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 632.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 571.06 MiB is free. Process 1096895 has 14.22 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 13.18 GiB memory in use. Process 1102113 has 14.69 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 11.77 GiB is allocated by PyTorch, and 906.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
841d9962,"{'latent_dim': 63, 'ae_topo_lambda': 1.461583208244995, 'ae_conv_num': 3, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.39696329222860516, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 676.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 373.06 MiB is free. Process 1096895 has 14.85 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.41 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.61 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
917cbcf9,"{'latent_dim': 27, 'ae_topo_lambda': 0.9908475727075132, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.16590524158217035, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 41.06 MiB is free. Process 1096895 has 14.85 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 14.13 GiB memory in use. Of the allocated memory 12.13 GiB is allocated by PyTorch, and 87.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
c48c0bbc,"{'latent_dim': 74, 'ae_topo_lambda': 1.6684096387189307, 'ae_conv_num': 2, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.09093809900922353, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 211.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.96 GiB memory in use. Of the allocated memory 10.71 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
fe82f950,"{'latent_dim': 41, 'ae_topo_lambda': 1.0350185979533435, 'ae_conv_num': 1, 'ae_conv_kernel': 2, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.22700713001407374, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 654.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 213.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.01 GiB memory in use. Process 1102473 has 13.96 GiB memory in use. Of the allocated memory 12.17 GiB is allocated by PyTorch, and 2.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
23c94c89,"{'latent_dim': 55, 'ae_topo_lambda': 1.32199906114652, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.19103448409584137, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 676.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.01 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 12.60 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
ca38410f,"{'latent_dim': 58, 'ae_topo_lambda': 1.3620688058900539, 'ae_conv_num': 1, 'ae_conv_kernel': 4, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.29934097960984746, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 203.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.58 GiB is allocated by PyTorch, and 1.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
4bf09432,"{'latent_dim': 68, 'ae_topo_lambda': 1.6026885922971934, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.3090684345160757, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 201.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.96 GiB is allocated by PyTorch, and 1.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
5188a595,"{'latent_dim': 71, 'ae_topo_lambda': 1.1119245473728157, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.02145671635304766, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 211.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 11.73 GiB is allocated by PyTorch, and 488.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
dccb4b9a,"{'latent_dim': 89, 'ae_topo_lambda': 0.7149672607874736, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.3849310886335099, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 211.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.98 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
6b9e5663,"{'latent_dim': 61, 'ae_topo_lambda': 1.2022211265968907, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.16256827054203032, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 203.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.95 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
f6f464db,"{'latent_dim': 49, 'ae_topo_lambda': 1.2445623943490638, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.14209675622833076, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 203.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
d5da4305,"{'latent_dim': 51, 'ae_topo_lambda': 1.0752131296562082, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.20230508934846217, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
5235a8d8,"{'latent_dim': 53, 'ae_topo_lambda': 1.3466288899767873, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.17921960636804582, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
8ea88d62,"{'latent_dim': 48, 'ae_topo_lambda': 0.9788824140675354, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.17319628847121957, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
8ecae163,"{'latent_dim': 47, 'ae_topo_lambda': 1.6430162298788336, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.1348279798828142, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 203.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
058815aa,"{'latent_dim': 54, 'ae_topo_lambda': 1.1304678997049942, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.15480547822962137, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.95 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
b9f04a39,"{'latent_dim': 50, 'ae_topo_lambda': 1.428914307029386, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.09789034171169088, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
9f0f9811,"{'latent_dim': 46, 'ae_topo_lambda': 1.2505180132782918, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.11402574356688616, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 203.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
a44181ab,"{'latent_dim': 57, 'ae_topo_lambda': 1.0218139362164367, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.19866506149613056, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.95 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
c650420f,"{'latent_dim': 44, 'ae_topo_lambda': 1.5067228436385638, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.12877884002489848, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 203.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
da795e01,"{'latent_dim': 52, 'ae_topo_lambda': 1.3961213429670334, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.1864362285559394, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
f180bf28,"{'latent_dim': 40, 'ae_topo_lambda': 0.9370400165256131, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.16717526908433214, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.93 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
8d520504,"{'latent_dim': 56, 'ae_topo_lambda': 1.187723235938857, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.2193819504599482, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 203.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.95 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
d3943a6a,"{'latent_dim': 54, 'ae_topo_lambda': 1.5709668877529062, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.1481410419540821, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.95 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
571432a8,"{'latent_dim': 50, 'ae_topo_lambda': 1.2919618719735788, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.19215135117532767, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
982ecc50,"{'latent_dim': 42, 'ae_topo_lambda': 1.7127992675807944, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.1793430221363134, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.93 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
9c09a525,"{'latent_dim': 48, 'ae_topo_lambda': 1.0416845639892904, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.21004396488371624, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
23ad7eb2,"{'latent_dim': 45, 'ae_topo_lambda': 1.1370377797694013, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.1574816520997625, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 203.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.94 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
fd7526cf,"{'latent_dim': 62, 'ae_topo_lambda': 1.4505665058501678, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.10490014266087788, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 201.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.95 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
993a5a44,"{'latent_dim': 39, 'ae_topo_lambda': 0.885716161989009, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.17184608759007147, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 205.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.93 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
67453b73,"{'latent_dim': 60, 'ae_topo_lambda': 1.6223904884611156, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.2013128419086908, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 201.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.95 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
429a443f,"{'latent_dim': 56, 'ae_topo_lambda': 1.99127354673154, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.111701322057084, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 203.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.95 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
12992ab9,"{'latent_dim': 67, 'ae_topo_lambda': 1.419130310316942, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.1701993259796903, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 203.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.96 GiB is allocated by PyTorch, and 1.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
9452cc6d,"{'latent_dim': 56, 'ae_topo_lambda': 1.272994511567623, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.16007777023734665, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 203.06 MiB is free. Process 1096895 has 14.86 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 11.72 GiB is allocated by PyTorch, and 511.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
26da25b2,"{'latent_dim': 61, 'ae_topo_lambda': 1.0223925419851398, 'ae_conv_num': 3, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.07747348208295637, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 165.06 MiB is free. Process 1096895 has 14.88 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 11.73 GiB is allocated by PyTorch, and 511.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
4bc40e59,"{'latent_dim': 43, 'ae_topo_lambda': 0.8566263550331088, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.062807482791612, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 676.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 173.06 MiB is free. Process 1096895 has 14.88 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.01 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 12.59 GiB is allocated by PyTorch, and 1.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
38b0eec3,"{'latent_dim': 71, 'ae_topo_lambda': 1.6590652580531189, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.15230788009804824, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 189.06 MiB is free. Process 1096895 has 14.88 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.01 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 11.73 GiB is allocated by PyTorch, and 488.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
15cfa58f,"{'latent_dim': 67, 'ae_topo_lambda': 1.0375145558026, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.1956100507015821, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 318.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 181.06 MiB is free. Process 1096895 has 14.88 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.01 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 10.96 GiB is allocated by PyTorch, and 1.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
7a0e49d5,"{'latent_dim': 47, 'ae_topo_lambda': 1.287490318850838, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.2643222353056626, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 676.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 181.06 MiB is free. Process 1096895 has 14.88 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.74 GiB memory in use. Process 1102113 has 15.01 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 12.59 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
80ddf7df,"{'latent_dim': 75, 'ae_topo_lambda': 1.2174539099940482, 'ae_conv_num': 1, 'ae_conv_kernel': 3, 'ae_conv_stride': 1, 'ae_conv_padding': 1, 'ae_dropout': 0.1131993537100768, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 189.06 MiB is free. Process 1096895 has 14.88 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.01 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 11.74 GiB is allocated by PyTorch, and 484.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
bc624762,"{'latent_dim': 54, 'ae_topo_lambda': 1.4447483038996642, 'ae_conv_num': 1, 'ae_conv_kernel': 4, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.17570087502732723, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 167.06 MiB is free. Process 1096895 has 14.89 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.72 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 11.07 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
73638287,"{'latent_dim': 61, 'ae_topo_lambda': 1.7390452785038693, 'ae_conv_num': 1, 'ae_conv_kernel': 4, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.18791915463678813, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 165.06 MiB is free. Process 1096895 has 14.89 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.73 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 11.08 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
8639028c,"{'latent_dim': 53, 'ae_topo_lambda': 1.3167960560394802, 'ae_conv_num': 1, 'ae_conv_kernel': 4, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.14485943729115802, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 308.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 167.06 MiB is free. Process 1096895 has 14.89 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.72 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 11.07 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
e3dea6b1,"{'latent_dim': 49, 'ae_topo_lambda': 1.1970186227969406, 'ae_conv_num': 1, 'ae_conv_kernel': 4, 'ae_conv_stride': 1, 'ae_conv_padding': 0, 'ae_dropout': 0.14936934871118895, 'ae_fc_num': 3}",<class 'torch.cuda.OutOfMemoryError'>,"CUDA out of memory. Tried to allocate 306.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 167.06 MiB is free. Process 1096895 has 14.89 GiB memory in use. Process 1101359 has 22.34 GiB memory in use. Process 1101706 has 12.72 GiB memory in use. Process 1102113 has 15.02 GiB memory in use. Process 1102473 has 13.97 GiB memory in use. Of the allocated memory 11.07 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","  File ""/home/darlinne.soto/new_framework/ray-tune-search/hs_objective_function.py"", line 61, in default_objective_function
    data=basic_experiment_config

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 58, in h_search_unit
    experiment_result = run_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_experiment.py"", line 16, in run_experiment
    return functions[config_to_execute.metadata.experiment_type](

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 156, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 127, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 248, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae_remade.py"", line 111, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 385, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 76, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 166, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 316, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 579, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
