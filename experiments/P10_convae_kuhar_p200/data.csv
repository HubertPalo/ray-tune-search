,score,randomforest-100-accuracy (mean),randomforest-100-accuracy (std),randomforest-100-f1-score macro (mean),randomforest-100-f1-score macro (std),randomforest-100-f1-score weighted (mean),randomforest-100-f1-score weighted (std),KNN-5-accuracy (mean),KNN-5-accuracy (std),KNN-5-f1-score macro (mean),KNN-5-f1-score macro (std),KNN-5-f1-score weighted (mean),KNN-5-f1-score weighted (std),SVM-rbf-C1.0-accuracy (mean),SVM-rbf-C1.0-accuracy (std),SVM-rbf-C1.0-f1-score macro (mean),SVM-rbf-C1.0-f1-score macro (std),SVM-rbf-C1.0-f1-score weighted (mean),SVM-rbf-C1.0-f1-score weighted (std),num_params,num_trainable_params,timestamp,done,training_iteration,trial_id,date,time_this_iter_s,time_total_s,pid,hostname,node_ip,time_since_restore,iterations_since_restore,checkpoint_dir_name,config/batch_size,config/num_CL,config/size_CL,config/kernel,config/num_HL,config/latent_dim,config/opt_lr,config/opt_wd,logdir,error_type,error_message,error_traceback
0,0.16666666666666669,0.16666666666666669,2.7755575615628914e-17,0.047619047619047616,0.0,0.047619047619047616,0.0,0.16666666666666666,0.0,0.047619047619047616,0.0,0.047619047619047616,0.0,0.16666666666666666,0.0,0.047619047619047616,0.0,0.047619047619047616,0.0,8055722,8055722,1698019626,False,1,1199ba06,2023-10-23_00-07-06,21.84765076637268,21.84765076637268,531259,ff5996df3739,172.17.0.2,21.84765076637268,1,,64,6,4,3,5,308,0.0006867375726798403,4.610590451583194e-06,1199ba06,,,
1,0.5286384976525822,0.5286384976525822,0.007258039828047092,0.5393579352850576,0.00740397404443038,0.5393579352850578,0.007403974044430422,0.5187793427230047,0.0,0.5255151059219488,0.0,0.5255151059219488,0.0,0.4413145539906103,0.0,0.4062789037349677,0.0,0.4062789037349678,0.0,18388892,18388892,1698019678,False,1,990f1bc7,2023-10-23_00-07-58,51.85336947441101,51.85336947441101,531259,ff5996df3739,172.17.0.2,51.85336947441101,1,,128,6,9,3,3,119,0.00012973583688183807,5.15325378397159e-07,990f1bc7,,,
2,0.7133802816901408,0.7133802816901408,0.008162130821931316,0.7109990165701106,0.008498782800887275,0.7109990165701106,0.00849878280088728,0.4765258215962441,0.0,0.43728570098623676,0.0,0.43728570098623676,0.0,0.6314553990610329,0.0,0.6302677761183068,0.0,0.6302677761183068,0.0,90428857,90428857,1698019959,False,1,929fc5d0,2023-10-23_00-12-39,301.1346139907837,301.1346139907837,531414,ff5996df3739,172.17.0.2,301.1346139907837,1,,128,3,25,3,2,658,1.938679326661043e-05,2.388643838645075e-06,929fc5d0,,,
3,0.5347417840375587,0.5347417840375587,0.010849032871635049,0.5087932439567894,0.009844901165456676,0.5087932439567894,0.009844901165456678,0.49061032863849763,0.0,0.4643113330387481,0.0,0.46431133303874816,0.0,0.31220657276995306,0.0,0.2809688330199544,0.0,0.28096883301995434,0.0,24456033,24456033,1698019691,False,1,3fd49e1a,2023-10-23_00-08-11,12.780463457107544,12.780463457107544,531259,ff5996df3739,172.17.0.2,12.780463457107544,1,,64,7,7,2,6,65,1.7428248901433657e-05,4.434567151586215e-06,3fd49e1a,,,
4,0.16666666666666669,0.16666666666666669,2.7755575615628914e-17,0.047619047619047616,0.0,0.047619047619047616,0.0,0.12910798122065728,0.0,0.11397694515002214,0.0,0.11397694515002216,0.0,0.16666666666666666,0.0,0.047619047619047616,0.0,0.047619047619047616,0.0,19393811,19393811,1698019707,True,1,c051cb83,2023-10-23_00-08-27,16.21739363670349,16.21739363670349,531259,ff5996df3739,172.17.0.2,16.21739363670349,1,,128,3,5,2,6,685,0.00028346012047205564,4.8767656588690645e-06,c051cb83,,,
5,0.3868544600938967,0.3868544600938967,0.008770676850830703,0.3209769071066053,0.007822565653523832,0.3209769071066053,0.007822565653523834,0.3826291079812207,0.0,0.3122614167217162,0.0,0.3122614167217162,0.0,0.3192488262910798,0.0,0.25596183390100663,0.0,0.25596183390100663,0.0,32193547,32193547,1698019760,True,1,d1168697,2023-10-23_00-09-20,58.13595128059387,58.13595128059387,531547,ff5996df3739,172.17.0.2,58.13595128059387,1,,64,5,8,4,5,649,0.0005786426397990724,7.339916884997645e-07,d1168697,,,
6,0.5516431924882629,0.5422535211267606,0.007855962690460806,0.5509632606240882,0.007826379950687226,0.5509632606240882,0.00782637995068721,0.5516431924882629,0.0,0.5545628321775308,0.0,0.5545628321775308,0.0,0.4694835680751174,0.0,0.36726571292848326,0.0,0.3672657129284832,0.0,51111699,51111699,1698019726,False,1,3db7e763,2023-10-23_00-08-46,19.444997310638428,19.444997310638428,531259,ff5996df3739,172.17.0.2,19.444997310638428,1,,128,3,9,4,7,325,0.000887969234814815,2.5698068789857923e-06,3db7e763,,,
7,0.16666666666666669,0.16666666666666669,2.7755575615628914e-17,0.047619047619047616,0.0,0.047619047619047616,0.0,0.16666666666666666,0.0,0.047619047619047616,0.0,0.047619047619047616,0.0,0.16666666666666666,0.0,0.047619047619047616,0.0,0.047619047619047616,0.0,546518800,546518800,1698019799,True,1,954a596d,2023-10-23_00-09-59,72.6631715297699,72.6631715297699,531259,ff5996df3739,172.17.0.2,72.6631715297699,1,,64,4,30,3,7,674,0.00012224019470511767,3.6647307978654754e-06,954a596d,,,
8,-0.1,,,,,,,,,,,,,,,,,,,-1,-1,1698019781,True,1,2ecadd94,2023-10-23_00-09-41,20.774498462677002,20.774498462677002,531547,ff5996df3739,172.17.0.2,20.774498462677002,1,,128,6,28,4,5,714,8.302123540850002e-05,6.6056149846607e-06,2ecadd94,<class 'torch.cuda.OutOfMemoryError'>,CUDA out of memory. Tried to allocate 180.00 MiB (GPU 0; 23.65 GiB total capacity; 1.63 GiB already allocated; 175.94 MiB free; 1.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF,"  File ""/home/darlinne.soto/new_framework/ray-tune-search/hyperparameters_search.py"", line 119, in my_objective_function
    result = h_search_unit(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 23, in h_search_unit
    experiment_result = run_basic_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 151, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 121, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 165, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 76, in __one_epoch
    loss.backward()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/_tensor.py"", line 487, in backward
    torch.autograd.backward(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/autograd/__init__.py"", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
"
9,0.16666666666666669,0.16666666666666669,2.7755575615628914e-17,0.047619047619047616,0.0,0.047619047619047616,0.0,0.1619718309859155,0.0,0.1382092706103927,0.0,0.1382092706103927,0.0,0.16666666666666666,0.0,0.047619047619047616,0.0,0.047619047619047616,0.0,15276284,15276284,1698019799,True,1,3764901f,2023-10-23_00-09-59,16.35727047920227,16.35727047920227,531547,ff5996df3739,172.17.0.2,16.35727047920227,1,,64,7,6,3,4,584,0.00040969098423420026,1.1488461416232054e-06,3764901f,,,
10,0.16666666666666669,0.16666666666666669,2.7755575615628914e-17,0.047619047619047616,0.0,0.047619047619047616,0.0,0.16666666666666666,0.0,0.047619047619047616,0.0,0.047619047619047616,0.0,0.16666666666666666,0.0,0.047619047619047616,0.0,0.047619047619047616,0.0,473688396,473688396,1698019853,True,1,38c48ffa,2023-10-23_00-10-53,54.0298855304718,54.0298855304718,531259,ff5996df3739,172.17.0.2,54.0298855304718,1,,64,6,28,2,7,486,0.000439005373508181,2.319260188125222e-06,38c48ffa,,,
11,0.5593896713615024,0.5593896713615024,0.007646242945963521,0.5600286227161144,0.008405380096356203,0.5600286227161144,0.008405380096356241,0.4225352112676056,0.0,0.3918289480254696,0.0,0.3918289480254696,0.0,0.44835680751173707,0.0,0.43795599946418423,0.0,0.43795599946418423,0.0,3879286,3879286,1698019840,False,1,6d3e9b07,2023-10-23_00-10-40,40.84232258796692,40.84232258796692,531547,ff5996df3739,172.17.0.2,40.84232258796692,1,,128,3,5,3,2,199,0.0005504903354052933,4.5304333697126056e-07,6d3e9b07,,,
12,0.5713615023474178,0.5713615023474178,0.010403060941859313,0.5402837756056996,0.012807356436091379,0.5402837756056996,0.012807356436091403,0.5633802816901409,0.0,0.5137949986879787,0.0,0.5137949986879787,0.0,0.4859154929577465,0.0,0.42964384006589,0.0,0.42964384006589007,0.0,35078174,35078174,1698019863,False,1,b23f07d9,2023-10-23_00-11-03,22.696967124938965,22.696967124938965,531547,ff5996df3739,172.17.0.2,22.696967124938965,1,,128,8,9,3,5,357,2.1385007044233933e-05,3.5368446285020596e-06,b23f07d9,,,
13,0.16666666666666669,0.16666666666666669,2.7755575615628914e-17,0.047619047619047616,0.0,0.047619047619047616,0.0,0.16666666666666666,0.0,0.047619047619047616,0.0,0.047619047619047616,0.0,0.16666666666666666,0.0,0.047619047619047616,0.0,0.047619047619047616,0.0,16965431,16965431,1698019876,True,1,342761f7,2023-10-23_00-11-16,22.87480854988098,22.87480854988098,531259,ff5996df3739,172.17.0.2,22.87480854988098,1,,64,7,5,4,6,607,0.000176416730417825,5.620148832049034e-06,342761f7,,,
14,-0.1,,,,,,,,,,,,,,,,,,,-1,-1,1698019864,True,1,5f4acd82,2023-10-23_00-11-04,0.7886989116668701,0.7886989116668701,531547,ff5996df3739,172.17.0.2,0.7886989116668701,1,,128,6,18,3,3,619,3.7620843395570866e-05,8.006722742449096e-06,5f4acd82,<class 'torch.cuda.OutOfMemoryError'>,CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 23.65 GiB total capacity; 1.61 GiB already allocated; 19.94 MiB free; 1.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF,"  File ""/home/darlinne.soto/new_framework/ray-tune-search/hyperparameters_search.py"", line 119, in my_objective_function
    result = h_search_unit(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 23, in h_search_unit
    experiment_result = run_basic_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 151, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 121, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 165, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 77, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 280, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 33, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 141, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 281, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 505, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
15,-0.1,,,,,,,,,,,,,,,,,,,-1,-1,1698019867,True,1,6df1ba31,2023-10-23_00-11-07,3.008207082748413,3.008207082748413,531547,ff5996df3739,172.17.0.2,3.008207082748413,1,,64,4,31,3,7,512,0.00038681814704526635,5.140698139523635e-06,6df1ba31,<class 'torch.cuda.OutOfMemoryError'>,CUDA out of memory. Tried to allocate 394.00 MiB (GPU 0; 23.65 GiB total capacity; 1.77 GiB already allocated; 25.94 MiB free; 1.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF,"  File ""/home/darlinne.soto/new_framework/ray-tune-search/hyperparameters_search.py"", line 119, in my_objective_function
    result = h_search_unit(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 23, in h_search_unit
    experiment_result = run_basic_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 151, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 121, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 129, in fit
    self.model = self.model.to(self.cuda_device)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1145, in to
    return self._apply(convert)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 797, in _apply
    module._apply(fn)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 797, in _apply
    module._apply(fn)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 797, in _apply
    module._apply(fn)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 820, in _apply
    param_applied = fn(param)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
"
16,-0.1,,,,,,,,,,,,,,,,,,,-1,-1,1698019868,True,1,f69faa39,2023-10-23_00-11-08,1.4271824359893799,1.4271824359893799,531547,ff5996df3739,172.17.0.2,1.4271824359893799,1,,128,6,19,3,7,376,8.625941543721618e-05,7.746397171542975e-07,f69faa39,<class 'torch.cuda.OutOfMemoryError'>,CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.65 GiB total capacity; 1.68 GiB already allocated; 23.94 MiB free; 1.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF,"  File ""/home/darlinne.soto/new_framework/ray-tune-search/hyperparameters_search.py"", line 119, in my_objective_function
    result = h_search_unit(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 23, in h_search_unit
    experiment_result = run_basic_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 151, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 121, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 165, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 76, in __one_epoch
    loss.backward()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/_tensor.py"", line 487, in backward
    torch.autograd.backward(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/autograd/__init__.py"", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
"
17,-0.1,,,,,,,,,,,,,,,,,,,-1,-1,1698019870,True,1,a7f2f24c,2023-10-23_00-11-10,1.6680538654327393,1.6680538654327393,531547,ff5996df3739,172.17.0.2,1.6680538654327393,1,,128,5,23,3,6,180,8.27786741043335e-05,5.566710548960288e-06,a7f2f24c,<class 'torch.cuda.OutOfMemoryError'>,CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 23.65 GiB total capacity; 1.59 GiB already allocated; 7.94 MiB free; 1.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF,"  File ""/home/darlinne.soto/new_framework/ray-tune-search/hyperparameters_search.py"", line 119, in my_objective_function
    result = h_search_unit(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 23, in h_search_unit
    experiment_result = run_basic_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 151, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 121, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 165, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 76, in __one_epoch
    loss.backward()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/_tensor.py"", line 487, in backward
    torch.autograd.backward(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/autograd/__init__.py"", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
"
18,0.6406103286384977,0.6406103286384977,0.007530053958847607,0.5854429850851575,0.0077111000013245475,0.5854429850851575,0.0077111000013245145,0.5845070422535211,0.0,0.537466366295695,0.0,0.537466366295695,0.0,0.44835680751173707,0.0,0.32228290879317273,0.0,0.3222829087931727,0.0,9449885,9449885,1698019903,False,1,4f82610b,2023-10-23_00-11-43,32.64598727226257,32.64598727226257,531547,ff5996df3739,172.17.0.2,32.64598727226257,1,,128,4,5,4,4,312,0.0003766340341759554,2.453477583631767e-06,4f82610b,,,
19,0.5988262910798122,0.5988262910798122,0.01030994894647783,0.6034053500712453,0.01021489958453866,0.6034053500712453,0.010214899584538679,0.5892018779342723,0.0,0.5935953453687735,0.0,0.5935953453687735,0.0,0.4835680751173709,0.0,0.4392056449040716,0.0,0.43920564490407166,0.0,138860899,138860899,1698019910,False,1,d4466e66,2023-10-23_00-11-50,33.86496114730835,33.86496114730835,531259,ff5996df3739,172.17.0.2,33.86496114730835,1,,128,3,18,3,5,369,1.3968077541587333e-05,5.002070273358306e-07,d4466e66,,,
20,-0.1,,,,,,,,,,,,,,,,,,,-1,-1,1698019904,True,1,3d757950,2023-10-23_00-11-44,0.993861198425293,0.993861198425293,531547,ff5996df3739,172.17.0.2,0.993861198425293,1,,128,2,13,2,4,218,4.404486851413666e-05,9.155585562785405e-06,3d757950,<class 'torch.cuda.OutOfMemoryError'>,CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.65 GiB total capacity; 1.67 GiB already allocated; 13.94 MiB free; 1.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF,"  File ""/home/darlinne.soto/new_framework/ray-tune-search/hyperparameters_search.py"", line 119, in my_objective_function
    result = h_search_unit(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 23, in h_search_unit
    experiment_result = run_basic_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 151, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 121, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 165, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 77, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 280, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 33, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 141, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 281, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 507, in _multi_tensor_adam
    denom = torch._foreach_add(exp_avg_sq_sqrt, eps)
"
21,0.5046948356807511,0.2875586854460094,0.002405387503746379,0.19330626957067848,0.0016992846754730504,0.1933062695706785,0.0016992846754730517,0.5046948356807511,0.0,0.4477797575237387,0.0,0.4477797575237386,0.0,0.3779342723004695,0.0,0.28140399371438946,0.0,0.28140399371438946,0.0,46303790,46303790,1698019941,True,1,007293ed,2023-10-23_00-12-21,36.519245624542236,36.519245624542236,531547,ff5996df3739,172.17.0.2,36.519245624542236,1,,128,8,12,4,4,428,3.2456773990167024e-05,3.3919205208938383e-06,007293ed,,,
22,-0.1,,,,,,,,,,,,,,,,,,,-1,-1,1698019942,True,1,24aba3af,2023-10-23_00-12-22,0.7764079570770264,0.7764079570770264,531547,ff5996df3739,172.17.0.2,0.7764079570770264,1,,128,2,16,4,4,423,0.00022898196761904024,1.5504145236023754e-06,24aba3af,<class 'torch.cuda.OutOfMemoryError'>,CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 23.65 GiB total capacity; 1.65 GiB already allocated; 71.94 MiB free; 1.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF,"  File ""/home/darlinne.soto/new_framework/ray-tune-search/hyperparameters_search.py"", line 119, in my_objective_function
    result = h_search_unit(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 23, in h_search_unit
    experiment_result = run_basic_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 151, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 121, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 165, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 77, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 280, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 33, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 141, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 281, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 505, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
23,-0.1,,,,,,,,,,,,,,,,,,,-1,-1,1698019943,True,1,d775aa7c,2023-10-23_00-12-23,0.9053032398223877,0.9053032398223877,531547,ff5996df3739,172.17.0.2,0.9053032398223877,1,,128,4,20,2,3,263,5.725729960157266e-05,1.422455901610613e-07,d775aa7c,<class 'torch.cuda.OutOfMemoryError'>,CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 23.65 GiB total capacity; 1.67 GiB already allocated; 111.94 MiB free; 1.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF,"  File ""/home/darlinne.soto/new_framework/ray-tune-search/hyperparameters_search.py"", line 119, in my_objective_function
    result = h_search_unit(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 23, in h_search_unit
    experiment_result = run_basic_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 151, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 121, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 165, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 77, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 280, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 33, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 141, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 281, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 442, in _multi_tensor_adam
    device_grads = torch._foreach_add(device_grads, device_params, alpha=weight_decay)
"
24,-0.1,,,,,,,,,,,,,,,,,,,-1,-1,1698019960,True,1,dfd261c3,2023-10-23_00-12-40,1.1137802600860596,1.1137802600860596,531414,ff5996df3739,172.17.0.2,1.1137802600860596,1,,128,4,22,2,4,15,0.0009034367484311261,2.8447848983223135e-08,dfd261c3,<class 'torch.cuda.OutOfMemoryError'>,CUDA out of memory. Tried to allocate 176.00 MiB (GPU 0; 23.65 GiB total capacity; 2.72 GiB already allocated; 149.94 MiB free; 2.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF,"  File ""/home/darlinne.soto/new_framework/ray-tune-search/hyperparameters_search.py"", line 119, in my_objective_function
    result = h_search_unit(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 23, in h_search_unit
    experiment_result = run_basic_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 151, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 121, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 165, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 77, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 280, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 33, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 141, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 281, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 442, in _multi_tensor_adam
    device_grads = torch._foreach_add(device_grads, device_params, alpha=weight_decay)
"
25,-0.1,,,,,,,,,,,,,,,,,,,-1,-1,1698019961,True,1,e2e19114,2023-10-23_00-12-41,0.9906706809997559,0.9906706809997559,531414,ff5996df3739,172.17.0.2,0.9906706809997559,1,,128,2,24,4,3,128,2.6477876535718368e-05,6.76990794650934e-06,e2e19114,<class 'torch.cuda.OutOfMemoryError'>,CUDA out of memory. Tried to allocate 186.00 MiB (GPU 0; 23.65 GiB total capacity; 2.77 GiB already allocated; 103.94 MiB free; 2.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF,"  File ""/home/darlinne.soto/new_framework/ray-tune-search/hyperparameters_search.py"", line 119, in my_objective_function
    result = h_search_unit(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/h_search_unit.py"", line 23, in h_search_unit
    experiment_result = run_basic_experiment(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/run_basic_experiment.py"", line 151, in run_basic_experiment
    datasets = do_reduce(

  File ""/home/darlinne.soto/new_framework/ray-tune-search/basic/do_reduce.py"", line 121, in do_reduce
    reducer.fit(**fit_dsets)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 165, in fit
    epoch_loss, epoch_ae_loss, epoch_topo_loss = self.__one_epoch(train_data_loader, train_mode=True)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/librep/transforms/topo_ae.py"", line 77, in __one_epoch
    self.optimizer.step()

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 280, in wrapper
    out = func(*args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/optimizer.py"", line 33, in _use_grad
    ret = func(self, *args, **kwargs)

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 141, in step
    adam(

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 281, in adam
    func(params,

  File ""/home/darlinne.soto/.local/lib/python3.10/site-packages/torch/optim/adam.py"", line 505, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
"
